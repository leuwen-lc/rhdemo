# CloudNativePG - FAQ

## Questions fr√©quentes sur CloudNativePG pour rhDemo

---

## üîß Configuration et Architecture

### Q1 : Peut-on utiliser CloudNativePG SANS haute disponibilit√© ?

**R√©ponse : OUI, c'est m√™me recommand√© pour staging !**

CloudNativePG supporte tr√®s bien le mode **single instance** (1 seule base) :

```yaml
spec:
  instances: 1  # Pas de r√©plication = pas de HA
```

**Ce que tu gardes avec 1 instance** :
- ‚úÖ Backups automatiques quotidiens
- ‚úÖ Point-In-Time Recovery (PITR)
- ‚úÖ PgBouncer (pooling de connexions)
- ‚úÖ Monitoring Prometheus complet
- ‚úÖ Gestion automatis√©e du cycle de vie
- ‚úÖ Restauration depuis backup

**Ce que tu n'as pas avec 1 instance** :
- ‚ùå Basculement automatique si le pod crash
- ‚ùå Lecture distribu√©e sur plusieurs replicas

**Consommation ressources** :
- Single instance : ~512 MB RAM, 0.5 CPU
- HA (3 instances) : ~1.5 GB RAM, 1.5 CPU

**Configuration recommand√©e pour stagingkub** :
- **Keycloak** : 1 instance (les realms changent rarement)
- **RHDemo** : 1 instance (environnement de test)

### Q2 : Peut-on activer la HA plus tard ?

**R√©ponse : OUI, sans migration !**

Tu peux passer de 1 √† 3 instances √† tout moment :

```bash
# √âditer le Cluster
kubectl patch cluster postgresql-keycloak -n rhdemo-stagingkub \
  --type merge -p '{"spec":{"instances":3}}'

# CloudNativePG cr√©e automatiquement 2 replicas
# et les synchronise avec le primary (sans downtime)
```

---

## üìä Monitoring et Grafana

### Q3 : Peut-on int√©grer CloudNativePG dans Grafana ?

**R√©ponse : OUI, compl√®tement !**

CloudNativePG expose **plus de 50 m√©triques Prometheus** natives.

**Dashboard cr√©√©** : [grafana-dashboard-cnpg.json](../infra/stagingkub/helm/observability/grafana-dashboard-cnpg.json)

**M√©triques disponibles** :
- `cnpg_pg_up` - Status du cluster (UP/DOWN)
- `cnpg_backends_total` - Connexions actives
- `cnpg_pg_database_size_bytes` - Taille des bases
- `cnpg_pg_stat_database_xact_commit` - Transactions committ√©es
- `cnpg_pg_backup_last_available_timestamp` - √Çge dernier backup
- `cnpg_pg_backup_duration_seconds` - Dur√©e des backups
- `cnpg_pg_wal_files` - Fichiers WAL
- `cnpg_pg_replication_lag` - Lag r√©plication (si HA)

**Panneaux du dashboard** :
1. Status des clusters (gauges UP/DOWN)
2. Connexions actives par cluster
3. Taille des bases de donn√©es
4. Transactions (commits/rollbacks)
5. √Çge du dernier backup (alertes si > 24h)
6. Fichiers WAL (Write-Ahead Log)
7. Dur√©e des backups

**Int√©gration automatique** :
```yaml
# CloudNativePG cr√©e automatiquement un PodMonitor
# Si Prometheus Operator est install√©, les m√©triques
# sont scrap√©es automatiquement

apiVersion: postgresql.cnpg.io/v1
kind: Cluster
spec:
  monitoring:
    enablePodMonitor: true  # ‚≠ê Active le scraping auto
```

**Acc√®s aux m√©triques** :
```bash
# Port-forward vers les m√©triques
kubectl port-forward -n rhdemo-stagingkub postgresql-keycloak-1 9187:9187

# Consulter les m√©triques
curl http://localhost:9187/metrics
```

### Q4 : Le dashboard sera-t-il charg√© automatiquement dans Grafana ?

**R√©ponse : Oui, via ConfigMap !**

Le dashboard `grafana-dashboard-cnpg.json` peut √™tre charg√© automatiquement :

**M√©thode 1 : Via sidecar (recommand√©)** :
```yaml
# Dans le Helm chart Grafana (loki-stack)
grafana:
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      folder: /tmp/dashboards
      provider:
        folder: CloudNativePG
```

Cr√©er une ConfigMap avec le dashboard :
```bash
kubectl create configmap grafana-dashboard-cnpg \
  --from-file=grafana-dashboard-cnpg.json \
  -n loki-stack \
  --dry-run=client -o yaml | kubectl label -f - grafana_dashboard=1 --local -o yaml | kubectl apply -f -
```

**M√©thode 2 : Import manuel** :
1. Acc√©der √† Grafana : http://localhost:3000
2. Menu : Dashboards ‚Üí Import
3. Uploader `grafana-dashboard-cnpg.json`

---

## üíæ Backups et Restauration

### Q5 : Les backups fonctionnent-ils sans HA ?

**R√©ponse : OUI, totalement ind√©pendant !**

Les backups automatiques **ne n√©cessitent pas la HA** :

```yaml
spec:
  instances: 1  # Single instance

  backup:
    barmanObjectStore:
      destinationPath: /backups/keycloak
      wal:
        compression: gzip
    retentionPolicy: "7d"
```

**Schedule automatique** :
```yaml
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: postgresql-keycloak-backup
spec:
  schedule: "0 2 * * *"  # Tous les jours √† 2h
  cluster:
    name: postgresql-keycloak
```

**Backups cr√©√©s** :
- Base backup complet (quotidien)
- WAL archiv√©s en continu (permet PITR)
- Compression gzip automatique
- R√©tention 7 jours (configurable)

### Q6 : Comment restaurer depuis un backup ?

**R√©ponse : Via un nouveau Cluster** :

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgresql-keycloak-restored
spec:
  instances: 1

  bootstrap:
    recovery:
      backup:
        name: <backup-name>  # Nom du backup

      # Ou Point-In-Time Recovery
      recoveryTarget:
        targetTime: "2026-01-10 14:00:00+00"
```

---

## üîÑ Migration et Cohabitation

### Q7 : Peut-on garder les StatefulSets actuels pendant la migration ?

**R√©ponse : OUI, c'est la strat√©gie recommand√©e !**

**Migration progressive** :
1. Installer l'op√©rateur CloudNativePG (pas d'impact sur StatefulSets)
2. Cr√©er le Cluster CloudNativePG Keycloak (coexiste)
3. Migrer les donn√©es de StatefulSet ‚Üí CloudNativePG
4. Tester pendant 2-3 jours
5. Si OK : supprimer l'ancien StatefulSet
6. R√©p√©ter pour PostgreSQL RHDemo

**Rollback facile** :
```bash
# Supprimer le Cluster CloudNativePG
kubectl delete cluster postgresql-keycloak -n rhdemo-stagingkub

# L'ancien StatefulSet peut √™tre recr√©√© via Helm
helm upgrade rhdemo ./helm/rhdemo \
  --set keycloak.cloudnativepg.enabled=false \
  --set keycloak.enabled=true
```

---

## üéØ Comparaisons

### Q8 : Quelle est la diff√©rence entre StatefulSet et CloudNativePG ?

| Crit√®re | StatefulSet actuel | CloudNativePG |
|---------|-------------------|---------------|
| **Backups** | ‚ùå Manuels | ‚úÖ Automatiques quotidiens |
| **PITR** | ‚ùå Non | ‚úÖ Oui (restaurer √† n'importe quel moment) |
| **HA** | ‚ùå Non | ‚ö†Ô∏è Optionnel (instances: 3) |
| **Pooling** | ‚ùå Non | ‚úÖ PgBouncer int√©gr√© |
| **Monitoring** | ‚ö†Ô∏è Basique | ‚úÖ 50+ m√©triques Prometheus |
| **Gestion** | üîß Scripts custom | ‚úÖ Op√©rateur automatis√© |
| **Complexit√©** | Faible | Moyenne |
| **Ressources (1 instance)** | ~400 MB | ~512 MB |

### Q9 : CloudNativePG vs Zalando Postgres Operator ?

| Crit√®re | CloudNativePG | Zalando PG Operator |
|---------|--------------|---------------------|
| **Projet** | CNCF Sandbox | Zalando |
| **Maturit√©** | R√©cent (2021), tr√®s actif | Ancien (2016), mature |
| **Backups** | Barman int√©gr√© | pgBackRest ou WAL-G |
| **PITR** | ‚úÖ Oui | ‚úÖ Oui |
| **Pooling** | PgBouncer natif | Pooler s√©par√© |
| **Configuration** | Plus simple | Plus verbeux |
| **Community** | Croissante | √âtablie |
| **Recommand√© pour** | Nouveau projet | Migration existant |

**Pourquoi CloudNativePG pour rhDemo ?**
- ‚úÖ Plus simple √† configurer
- ‚úÖ PgBouncer int√©gr√©
- ‚úÖ Backups Barman natifs
- ‚úÖ Projet CNCF (neutralit√©)
- ‚úÖ Documentation excellente

---

## üöÄ Mise en production

### Q10 : CloudNativePG est-il pr√™t pour la production ?

**R√©ponse : OUI, utilis√© en production par de grandes entreprises !**

**Adopteurs connus** :
- EDB (EnterpriseDB) - sponsor principal
- CNCF (projet Sandbox officiel)
- Nombreuses entreprises europ√©ennes

**Garanties** :
- ‚úÖ Tests automatis√©s complets
- ‚úÖ Releases r√©guli√®res (tous les 3 mois)
- ‚úÖ Support communautaire actif
- ‚úÖ Documentation compl√®te

**Version recommand√©e** :
- Utiliser la derni√®re version stable (actuellement 1.23+)
- Compatible PostgreSQL 12 ‚Üí 16

**Configuration production type** :
```yaml
instances: 3  # HA activ√©e
storage:
  size: 20Gi
  storageClass: fast-ssd  # SSD pour perfs
resources:
  requests:
    memory: 1Gi
    cpu: 500m
  limits:
    memory: 2Gi
    cpu: 2000m
backup:
  retentionPolicy: "30d"  # 30 jours en prod
pooler:
  instances: 3  # PgBouncer HA aussi
```

---

## üìö Ressources

- [CloudNativePG Documentation](https://cloudnative-pg.io/documentation/)
- [CloudNativePG GitHub](https://github.com/cloudnative-pg/cloudnative-pg)
- [Helm Charts](https://github.com/cloudnative-pg/charts)
- [Guide Migration rhDemo](./CLOUDNATIVEPG_MIGRATION.md)
- [Dashboard Grafana](../infra/stagingkub/helm/observability/grafana-dashboard-cnpg.json)

---

## üéØ R√©sum√© pour rhDemo stagingkub

**Configuration recommand√©e** :
```yaml
keycloak:
  cloudnativepg:
    enabled: true
    instances: 1  # ‚≠ê SANS HA pour staging
    backup:
      retentionPolicy: "7d"
    pooler:
      enabled: true
      instances: 1

rhdemo:
  cloudnativepg:
    enabled: true
    instances: 1  # ‚≠ê SANS HA pour staging
    backup:
      retentionPolicy: "7d"
    pooler:
      enabled: true
      instances: 1
```

**B√©n√©fices pour stagingkub** :
- ‚úÖ Persistance garantie (extraMounts + backups auto)
- ‚úÖ Recovery rapide (PITR)
- ‚úÖ Monitoring complet (Grafana)
- ‚úÖ Gestion simplifi√©e (op√©rateur)
- ‚úÖ Consommation raisonnable (~1 GB RAM total)
- ‚úÖ HA activable plus tard si besoin

---

**Derni√®re mise √† jour** : 2026-01-10
