# Migration vers CloudNativePG

## ğŸ“‹ Vue d'ensemble

Ce guide dÃ©crit la migration des StatefulSets PostgreSQL actuels vers l'opÃ©rateur **CloudNativePG** pour bÃ©nÃ©ficier de :
- âœ… Sauvegardes automatiques avec rÃ©tention configurable
- âœ… Point-In-Time Recovery (PITR)
- âœ… Haute disponibilitÃ© (replicas automatiques)
- âœ… Pooling de connexions intÃ©grÃ© (PgBouncer)
- âœ… Monitoring natif avec mÃ©triques Prometheus
- âœ… Gestion automatisÃ©e du cycle de vie

## ğŸ¯ Architecture cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CloudNativePG Operator                                         â”‚
â”‚  â”œâ”€> Cluster postgresql-keycloak                              â”‚
â”‚  â”‚    â”œâ”€ Primary Pod (RW)                                     â”‚
â”‚  â”‚    â”œâ”€ Replica Pod (RO) [optionnel]                        â”‚
â”‚  â”‚    â”œâ”€ PgBouncer (pooling)                                  â”‚
â”‚  â”‚    â””â”€ ScheduledBackup (quotidien)                         â”‚
â”‚  â”‚        â””â”€> PVC backups-keycloak/                          â”‚
â”‚  â”‚                                                             â”‚
â”‚  â””â”€> Cluster postgresql-rhdemo                                â”‚
â”‚       â”œâ”€ Primary Pod (RW)                                     â”‚
â”‚       â”œâ”€ Replica Pod (RO) [optionnel]                        â”‚
â”‚       â”œâ”€ PgBouncer (pooling)                                  â”‚
â”‚       â””â”€ ScheduledBackup (quotidien)                         â”‚
â”‚            â””â”€> PVC backups-rhdemo/                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ PrÃ©requis

### 1. Persistance des donnÃ©es configurÃ©e

Avant de migrer, **assure-toi que les extraMounts KinD sont configurÃ©s** :

```bash
# VÃ©rifier que kind-config.yaml contient extraMounts
cat /home/leno-vo/git/repository/rhDemo/infra/stagingkub/kind-config.yaml | grep -A 5 extraMounts

# Si le cluster existe dÃ©jÃ  SANS extraMounts, il faut le recrÃ©er
kind delete cluster --name rhdemo
cd /home/leno-vo/git/repository/rhDemo/infra/stagingkub/scripts
./init-stagingkub.sh
```

### 2. Sauvegarder les donnÃ©es existantes

Avant toute migration, **sauvegarde manuelle obligatoire** :

```bash
# Sauvegarde PostgreSQL Keycloak
kubectl exec -n rhdemo-stagingkub postgresql-keycloak-0 -- \
  pg_dump -U keycloak keycloak > /tmp/keycloak-backup-$(date +%Y%m%d-%H%M%S).sql

# Sauvegarde PostgreSQL RHDemo
kubectl exec -n rhdemo-stagingkub postgresql-rhdemo-0 -- \
  pg_dump -U dbrhdemo dbrhdemo > /tmp/rhdemo-backup-$(date +%Y%m%d-%H%M%S).sql
```

## ğŸš€ Phase 1 : Installation de CloudNativePG Operator

### Ã‰tape 1.1 : Installer l'opÃ©rateur

```bash
# Ajouter le repository Helm CloudNativePG
helm repo add cnpg https://cloudnative-pg.github.io/charts
helm repo update

# CrÃ©er le namespace pour l'opÃ©rateur
kubectl create namespace cnpg-system

# Installer l'opÃ©rateur CloudNativePG
helm install cnpg \
  --namespace cnpg-system \
  cnpg/cloudnative-pg \
  --set monitoring.enabled=true
```

### Ã‰tape 1.2 : VÃ©rifier l'installation

```bash
# VÃ©rifier que l'opÃ©rateur est Running
kubectl get pods -n cnpg-system

# VÃ©rifier les CRDs crÃ©Ã©es
kubectl get crd | grep postgresql.cnpg.io

# Devrait afficher :
# - clusters.postgresql.cnpg.io
# - backups.postgresql.cnpg.io
# - scheduledbackups.postgresql.cnpg.io
# - poolers.postgresql.cnpg.io
```

## ğŸ”„ Phase 2 : Migration PostgreSQL Keycloak

### Ã‰tape 2.1 : CrÃ©er le Cluster CloudNativePG pour Keycloak

CrÃ©er le fichier [infra/stagingkub/helm/rhdemo/templates/cnpg-cluster-keycloak.yaml](../infra/stagingkub/helm/rhdemo/templates/cnpg-cluster-keycloak.yaml) :

```yaml
{{- if .Values.keycloak.cloudnativepg.enabled }}
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgresql-keycloak
  namespace: {{ .Values.global.namespace }}
  labels:
    app: postgresql-keycloak
spec:
  # â­ 1 instance = pas de HA (recommandÃ© pour staging)
  # Augmenter Ã  3 pour activer la haute disponibilitÃ© plus tard
  instances: {{ .Values.keycloak.cloudnativepg.instances }}

  imageName: ghcr.io/cloudnative-pg/postgresql:16

  storage:
    size: {{ .Values.keycloak.cloudnativepg.storage.size }}
    storageClass: {{ .Values.keycloak.cloudnativepg.storage.class }}

  bootstrap:
    initdb:
      database: keycloak
      owner: keycloak
      secret:
        name: {{ .Values.keycloak.database.passwordSecret.name }}

  monitoring:
    enablePodMonitor: true

  # Configuration PostgreSQL
  postgresql:
    parameters:
      max_connections: "200"
      shared_buffers: "256MB"
      effective_cache_size: "1GB"
      work_mem: "16MB"

  # Sauvegardes automatiques
  backup:
    barmanObjectStore:
      destinationPath: {{ .Values.keycloak.cloudnativepg.backup.destinationPath }}
      wal:
        compression: gzip
        maxParallel: 2
      data:
        compression: gzip
        immediateCheckpoint: true
    retentionPolicy: "{{ .Values.keycloak.cloudnativepg.backup.retentionPolicy }}"

  # PgBouncer pour pooling de connexions
  pooler:
    enabled: {{ .Values.keycloak.cloudnativepg.pooler.enabled }}
    instances: {{ .Values.keycloak.cloudnativepg.pooler.instances }}
    type: rw
    pgbouncer:
      poolMode: session
      parameters:
        max_client_conn: "1000"
        default_pool_size: "25"
{{- end }}
```

### Ã‰tape 2.2 : CrÃ©er le ScheduledBackup

CrÃ©er [infra/stagingkub/helm/rhdemo/templates/cnpg-scheduled-backup-keycloak.yaml](../infra/stagingkub/helm/rhdemo/templates/cnpg-scheduled-backup-keycloak.yaml) :

```yaml
{{- if .Values.keycloak.cloudnativepg.enabled }}
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: postgresql-keycloak-backup
  namespace: {{ .Values.global.namespace }}
spec:
  schedule: "0 2 * * *"  # Tous les jours Ã  2h du matin
  backupOwnerReference: self
  cluster:
    name: postgresql-keycloak
  immediate: true  # PremiÃ¨re sauvegarde immÃ©diate aprÃ¨s crÃ©ation
{{- end }}
```

### Ã‰tape 2.3 : Ajouter les valeurs dans values.yaml

Modifier [infra/stagingkub/helm/rhdemo/values.yaml](../infra/stagingkub/helm/rhdemo/values.yaml) :

```yaml
keycloak:
  # DÃ©sactiver l'ancien StatefulSet
  enabled: false  # âš ï¸ Ã€ mettre Ã  false aprÃ¨s migration

  # Configuration CloudNativePG
  cloudnativepg:
    enabled: true
    instances: 1  # â­ 1 = SANS HA (recommandÃ© staging), 3 = AVEC HA (prod)

    storage:
      size: 2Gi
      class: standard

    backup:
      # PVC local pour les backups
      destinationPath: s3://backups-keycloak
      retentionPolicy: "7d"  # Garder 7 jours de backups

    pooler:
      enabled: true
      instances: 1

  # Adapter le service pour pointer vers CloudNativePG
  database:
    host: postgresql-keycloak-rw  # Service crÃ©Ã© par CloudNativePG
    port: 5432
    name: keycloak
    user: keycloak
    passwordSecret:
      name: postgresql-keycloak-secret
      key: password
```

### Ã‰tape 2.4 : Migrer les donnÃ©es

```bash
# 1. Scaler down Keycloak (Ã©viter les Ã©critures)
kubectl scale deployment keycloak -n rhdemo-stagingkub --replicas=0

# 2. Faire une derniÃ¨re sauvegarde complÃ¨te
kubectl exec -n rhdemo-stagingkub postgresql-keycloak-0 -- \
  pg_dump -U keycloak keycloak > /tmp/keycloak-final-backup.sql

# 3. DÃ©ployer le nouveau cluster CloudNativePG
helm upgrade rhdemo \
  /home/leno-vo/git/repository/rhDemo/infra/stagingkub/helm/rhdemo \
  -n rhdemo-stagingkub \
  --set keycloak.enabled=false \
  --set keycloak.cloudnativepg.enabled=true

# 4. Attendre que le cluster soit prÃªt
kubectl wait --for=condition=ready cluster/postgresql-keycloak -n rhdemo-stagingkub --timeout=5m

# 5. Restaurer les donnÃ©es
kubectl exec -i -n rhdemo-stagingkub postgresql-keycloak-1 -- \
  psql -U keycloak keycloak < /tmp/keycloak-final-backup.sql

# 6. RedÃ©marrer Keycloak avec la nouvelle config
kubectl scale deployment keycloak -n rhdemo-stagingkub --replicas=1

# 7. VÃ©rifier la connexion
kubectl logs -n rhdemo-stagingkub deployment/keycloak --tail=50
```

### Ã‰tape 2.5 : Supprimer l'ancien StatefulSet

Une fois que tout fonctionne :

```bash
# Supprimer l'ancien StatefulSet (mais GARDER le PVC pour le moment)
kubectl delete statefulset postgresql-keycloak -n rhdemo-stagingkub

# VÃ©rifier que le nouveau cluster fonctionne bien pendant 1-2 jours

# Ensuite, supprimer l'ancien PVC
kubectl delete pvc postgresql-data-postgresql-keycloak-0 -n rhdemo-stagingkub
```

## ğŸ”„ Phase 3 : Migration PostgreSQL RHDemo

RÃ©pÃ©ter les mÃªmes Ã©tapes pour PostgreSQL RHDemo :

1. CrÃ©er `cnpg-cluster-rhdemo.yaml`
2. CrÃ©er `cnpg-scheduled-backup-rhdemo.yaml`
3. Ajouter `rhdemo.cloudnativepg` dans `values.yaml`
4. Scaler down l'application
5. Migrer les donnÃ©es
6. VÃ©rifier et supprimer l'ancien StatefulSet

## ğŸ“Š Phase 4 : Configuration des backups

### VÃ©rifier les backups automatiques

```bash
# Lister les backups Keycloak
kubectl get backup -n rhdemo-stagingkub -l cnpg.io/cluster=postgresql-keycloak

# Voir les dÃ©tails d'un backup
kubectl describe backup <backup-name> -n rhdemo-stagingkub

# Voir le schedule
kubectl get scheduledbackup -n rhdemo-stagingkub
```

### Restauration manuelle d'un backup

```yaml
# restore-keycloak.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgresql-keycloak-restored
  namespace: rhdemo-stagingkub
spec:
  instances: 1
  storage:
    size: 2Gi

  bootstrap:
    recovery:
      backup:
        name: <backup-name>  # Nom du backup Ã  restaurer

      # Ou Point-In-Time Recovery
      # recoveryTarget:
      #   targetTime: "2026-01-10 14:00:00.000000+00"
```

## ğŸ” Monitoring et maintenance

### Dashboards Grafana

CloudNativePG expose des mÃ©triques Prometheus qui peuvent Ãªtre visualisÃ©es dans Grafana.

#### Dashboard dÃ©diÃ© CloudNativePG

Un dashboard Grafana spÃ©cifique a Ã©tÃ© crÃ©Ã© : [grafana-dashboard-cnpg.json](../infra/stagingkub/helm/observability/grafana-dashboard-cnpg.json)

**Panneaux inclus** :
- âœ… Status des clusters (UP/DOWN)
- âœ… Connexions actives par cluster
- âœ… Taille des bases de donnÃ©es
- âœ… Transactions (commits/rollbacks)
- âœ… Ã‚ge du dernier backup
- âœ… Fichiers WAL (Write-Ahead Log)
- âœ… DurÃ©e des backups

#### DÃ©ploiement du dashboard

Le dashboard sera automatiquement chargÃ© par Grafana via ConfigMap :

```bash
# Le dashboard est dÃ©jÃ  dans infra/stagingkub/helm/observability/
# Il sera chargÃ© automatiquement au dÃ©ploiement de la stack Loki

# AccÃ¨s Grafana
# https://grafana.stagingkub.local (si configurÃ©)
# ou port-forward :
kubectl port-forward -n loki-stack svc/loki-grafana 3000:80
# http://localhost:3000
```

### MÃ©triques Prometheus

CloudNativePG expose automatiquement des mÃ©triques :

```bash
# Port-forward vers les mÃ©triques
kubectl port-forward -n rhdemo-stagingkub postgresql-keycloak-1 9187:9187

# AccÃ©der aux mÃ©triques
curl http://localhost:9187/metrics
```

MÃ©triques clÃ©s :
- `cnpg_pg_replication_lag` : Lag de rÃ©plication
- `cnpg_pg_database_size_bytes` : Taille de la base
- `cnpg_backends_waiting_total` : Connexions en attente

### VÃ©rifier la santÃ© du cluster

```bash
# Status gÃ©nÃ©ral
kubectl get cluster -n rhdemo-stagingkub

# Logs du primary
kubectl logs -n rhdemo-stagingkub postgresql-keycloak-1 -f

# ExÃ©cuter des requÃªtes
kubectl exec -it -n rhdemo-stagingkub postgresql-keycloak-1 -- psql -U keycloak
```

## ğŸš¨ Rollback en cas de problÃ¨me

Si la migration Ã©choue :

```bash
# 1. Supprimer le cluster CloudNativePG
helm upgrade rhdemo \
  /home/leno-vo/git/repository/rhDemo/infra/stagingkub/helm/rhdemo \
  -n rhdemo-stagingkub \
  --set keycloak.cloudnativepg.enabled=false \
  --set keycloak.enabled=true

# 2. L'ancien StatefulSet sera recrÃ©Ã© avec les donnÃ©es intactes (PVC toujours lÃ )

# 3. Restaurer depuis le backup si nÃ©cessaire
kubectl exec -i -n rhdemo-stagingkub postgresql-keycloak-0 -- \
  psql -U keycloak keycloak < /tmp/keycloak-final-backup.sql
```

## ğŸ“š Ressources

- [CloudNativePG Documentation](https://cloudnative-pg.io/documentation/)
- [CloudNativePG Helm Chart](https://github.com/cloudnative-pg/charts)
- [PostgreSQL 16 Documentation](https://www.postgresql.org/docs/16/)

## âœ… Checklist de migration

### PrÃ©-migration
- [ ] extraMounts KinD configurÃ©s et testÃ©s
- [ ] Backups manuels crÃ©Ã©s et vÃ©rifiÃ©s
- [ ] Documentation lue et comprise
- [ ] Cluster de test crÃ©Ã© (optionnel)

### Migration Keycloak
- [ ] OpÃ©rateur CloudNativePG installÃ©
- [ ] Manifests Helm crÃ©Ã©s
- [ ] Values.yaml configurÃ©
- [ ] Application arrÃªtÃ©e (scaled to 0)
- [ ] Backup final crÃ©Ã©
- [ ] Cluster CloudNativePG dÃ©ployÃ©
- [ ] DonnÃ©es restaurÃ©es
- [ ] Application redÃ©marrÃ©e
- [ ] Tests fonctionnels OK
- [ ] Backups automatiques vÃ©rifiÃ©s

### Migration RHDemo
- [ ] Manifests Helm crÃ©Ã©s
- [ ] Values.yaml configurÃ©
- [ ] Application arrÃªtÃ©e (scaled to 0)
- [ ] Backup final crÃ©Ã©
- [ ] Cluster CloudNativePG dÃ©ployÃ©
- [ ] DonnÃ©es restaurÃ©es
- [ ] Application redÃ©marrÃ©e
- [ ] Tests fonctionnels OK
- [ ] Backups automatiques vÃ©rifiÃ©s

### Post-migration
- [ ] Anciens StatefulSets supprimÃ©s
- [ ] Anciens PVC supprimÃ©s (aprÃ¨s pÃ©riode de sÃ©curitÃ©)
- [ ] Monitoring configurÃ© (Prometheus/Grafana)
- [ ] Documentation mise Ã  jour
- [ ] CLAUDE.md mis Ã  jour

## ğŸ¯ BÃ©nÃ©fices attendus

AprÃ¨s migration :
- âœ… **ZÃ©ro perte de donnÃ©es** mÃªme aprÃ¨s redÃ©marrage machine (extraMounts)
- âœ… **Backups automatiques** quotidiens avec rÃ©tention 7 jours
- âœ… **Point-In-Time Recovery** pour restaurer Ã  n'importe quel moment
- âœ… **Haute disponibilitÃ©** (replicas) prÃªt pour activation (optionnel)
- âœ… **Pooling de connexions** (PgBouncer) pour meilleures performances
- âœ… **Monitoring intÃ©grÃ©** avec mÃ©triques Prometheus + dashboards Grafana
- âœ… **Gestion simplifiÃ©e** : plus besoin de scripts custom

---

## ğŸ”§ Configuration Haute DisponibilitÃ© (Optionnel)

### Mode Single Instance (RecommandÃ© pour staging)

Par dÃ©faut, la configuration utilise **1 seule instance** :
- âœ… Consommation ressources rÃ©duite (~50%)
- âœ… Backups et PITR fonctionnels
- âœ… Monitoring complet
- âŒ Pas de basculement automatique si crash
- âŒ Pas de lecture distribuÃ©e

**Configuration** :
```yaml
instances: 1
```

### Mode Haute DisponibilitÃ© (Optionnel pour production)

Pour activer la HA, augmenter Ã  **3 instances** :
- âœ… Basculement automatique en cas de crash du primary
- âœ… Lecture distribuÃ©e sur replicas
- âœ… RÃ©plication synchrone
- âš ï¸ Consommation ressources Ã— 3

**Configuration** :
```yaml
instances: 3  # 1 primary + 2 replicas
```

**Architecture HA** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ postgresql-keycloak-1 (Primary - RW)      â”‚
â”‚  â”œâ”€> RÃ©plication synchrone                â”‚
â”‚  â†“                                         â”‚
â”‚ postgresql-keycloak-2 (Replica - RO)      â”‚
â”‚                                            â”‚
â”‚ postgresql-keycloak-3 (Replica - RO)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Services crÃ©Ã©s** :
- `postgresql-keycloak-rw` â†’ Primary (lecture/Ã©criture)
- `postgresql-keycloak-ro` â†’ Replicas (lecture seule)
- `postgresql-keycloak-r` â†’ Tous (lecture)

### Activer la HA plus tard (sans migration)

CloudNativePG permet de passer de 1 Ã  3 instances **sans downtime** :

```bash
# MÃ©thode 1 : Via kubectl patch
kubectl patch cluster postgresql-keycloak -n rhdemo-stagingkub \
  --type merge -p '{"spec":{"instances":3}}'

# MÃ©thode 2 : Via Helm values.yaml
# Ã‰diter values.yaml : instances: 3
helm upgrade rhdemo ./helm/rhdemo -n rhdemo-stagingkub

# CloudNativePG crÃ©e automatiquement les replicas
# et synchronise les donnÃ©es depuis le primary
```

**VÃ©rifier la rÃ©plication** :
```bash
# Status du cluster
kubectl get cluster postgresql-keycloak -n rhdemo-stagingkub

# VÃ©rifier les instances
kubectl get pods -n rhdemo-stagingkub -l cnpg.io/cluster=postgresql-keycloak

# Lag de rÃ©plication
kubectl exec -n rhdemo-stagingkub postgresql-keycloak-1 -- \
  psql -U keycloak -c "SELECT application_name, state, sync_state, replay_lag FROM pg_stat_replication;"
```

### Comparaison modes

| CritÃ¨re | Single (1 instance) | HA (3 instances) |
|---------|-------------------|------------------|
| **RAM** | ~512 MB | ~1.5 GB |
| **CPU** | 0.5 core | 1.5 cores |
| **Backups** | âœ… Oui | âœ… Oui |
| **PITR** | âœ… Oui | âœ… Oui |
| **Basculement auto** | âŒ Non | âœ… Oui (< 30s) |
| **Lecture distribuÃ©e** | âŒ Non | âœ… Oui |
| **ComplexitÃ©** | Faible | Moyenne |
| **RecommandÃ© pour** | Dev/Staging | Production |
